
I'm looking to analyze/quantify defense and defensive metrics

structure of my wd is data/ which has merged_cbba and merged_synergy

what i need from you is organization and overall structure of how i should approach this project.

consider the following data sources merged_synergy.csv and merged_cbba.csv
merged_synergy.csv has 600 instances and looks like : 
Player,Poss,PPP,PPS,Team,conf
Anna Eddy,0.8,1.417,1.5,Portland Pilots,WCC
Nadia Bernard,1.2,1.125,1.21,Saint Mary's Gaels,WCC
Jamie Kent,3.1,1.118,1.22,Pacific Tigers,WCC
Marina Hernandez,1.1,1.091,1.11,San Francisco Dons,WCC

merged_cbba has a little more than 900 instances.
both data sources should have a decent amount of overlap.

we might consider merging the data sources but the names and team names might have some differences.

now consider team name mappings and team defensive ratings which we want to include as a column in our data file 

import pandas as pd
from io import StringIO

a10_team_def_ratings = """
Team,TeamDefRtg
George Mason Patriots,85.8
Massachusetts Minutewomen,86.5
Rhode Island Rams, 87.7
Saint Joseph's Hawks, 88.0
Virginia Commonwealth Rams, 88.2
Richmond Spiders, 88.4 
Davidson Wildcats, 89.1
Fordham Rams, 89.6
Dayton Flyers, 89.6
Duquesne Dukes, 90.3
George Washington Revolutionaries
La Salle Explorers, 97.1
Loyola (Chicago) Ramblers, 97.2
Saint Louis Billikens, 99.7
St. Bonaventure Bonnies, 103.7
"""
a10_team_def_df = pd.read_csv(StringIO(a10_team_def_ratings))

a10_team_name_map = {
    # Atlantic 10 Conference Teams
    'VCU': 'Virginia Commonwealth Rams',
    'Virginia Commonwealth': 'Virginia Commonwealth Rams',
    'Massachusetts': 'Massachusetts Minutewomen',
    'UMass': 'Massachusetts Minutewomen',
    'Dayton': 'Dayton Flyers',
    'Fordham': 'Fordham Rams',
    'George Washington': 'George Washington Revolutionaries',
    'GW': 'George Washington Revolutionaries',
    'Davidson': 'Davidson Wildcats',
    'Richmond': 'Richmond Spiders',
    "Saint Joseph's": "Saint Joseph's Hawks",
    "St. Joseph's": "Saint Joseph's Hawks",
    'Rhode Island': 'Rhode Island Rams',
    'URI': 'Rhode Island Rams',
    'Loyola Chicago': 'Loyola (Chicago) Ramblers',
    'Loyola (Chicago)': 'Loyola (Chicago) Ramblers',
    'George Mason': 'George Mason Patriots',
    'GMU': 'George Mason Patriots',
    'St. Bonaventure': 'St. Bonaventure Bonnies',
    'Bonaventure': 'St. Bonaventure Bonnies',
    'Duquesne': 'Duquesne Dukes',
    'Saint Louis': 'Saint Louis Billikens',
    'SLU': 'Saint Louis Billikens',
    'La Salle': 'La Salle Explorers',
}
#------------------------------------
aac_team_def_ratings = """
Team,TeamDefRtg
Texas-(San Antonio) Roadrunners,84.4
North Texas Mean Green,85.3
East Carolina Pirats,87.1
Tulane Green Wave,87.5
Temple Owls,87.7
Tusla Golden Hurricane,88.4
Rice Owls,89.9
South FLorida Bulls,91.0
FLorida Atlantic,94.0
Wichita State Shockers,94.1
UAB Blazers,94.9
Charlotte 49ers,97.7
Memphis Tigers,105.0

"""
aac_team_def_ratings = pd.read_csv(StringIO(aac_team_def_ratings))
aac_team_name_map = {
    # Ordered exactly as requested
    'UTSA': 'Texas-(San Antonio) Roadrunners',
    'North Texas': 'North Texas Mean Green',
    'East Carolina': 'East Carolina Pirates',
    'Tulane': 'Tulane Green Wave',
    'Temple': 'Temple Owls',
    'Tulsa': 'Tulsa Golden Hurricane',
    'Rice': 'Rice Owls',
    'South Fla.': 'South Florida Bulls',
    'Fla. Atlantic': 'Florida Atlantic Owls',
    'Wichita St.': 'Wichita State Shockers',
    'UAB': 'UAB Blazers',
    'Charlotte': 'Charlotte 49ers',
    'Memphis': 'Memphis Tigers'
}
#------------------------------------
ivy_team_def_ratings = """
Team,TeamDefRtg
Harvard Crimson,80.7
Columbia Lions,88.1
Princeton Tigers,89.3
Pennsylvania Quakers, 94.1
Cornell Big Red, 95.4
Brown Bears, 96.5
Dartmouth Big Green, 99.7
Yale Bulldogs, 103.8
"""
ivy_team_def_ratings = pd.read_csv(StringIO(ivy_team_def_ratings))

ivy_team_name_map = {
    'Brown': 'Brown Bears',
    'Harvard': 'Harvard Crimson',
    'Yale': 'Yale Bulldogs',
    'Dartmouth': 'Dartmouth Big Green',
    'Columbia': 'Columbia Lions',
    'Penn': 'Pennsylvania Quakers',  # Note: 'Penn' is the common abbreviation
    'Cornell': 'Cornell Big Red',
    'Princeton': 'Princeton Tigers',
}
#------------------------------------
mvc_team_def_ratings = """
Team,TeamDefRtg
Missouri State Lady Bears,89.8
Belmont Bruins,91.2
Bradley Braves,91.3
Murray State Racers,95.2
Illinois-Chicago Flames,95.4
Drake Bulldogs,97.1
Northern Iowa Panthers,97.9
Valparaiso University,98.0
Illinois State Redbirds,98.5
Evansville Aces,102.0
Indiana State Sycamores,105.2
Southern Illinois Salukis,109.7

"""
mvc_team_def_df = pd.read_csv(StringIO(mvc_team_def_ratings))

#------------------------------------
mw_team_def_ratings = """
Team,TeamDefRtg
UNLV Rebels,91.1
San Diego State Aztecs,91.6
Wyoming Cowgirls,91.7
Air Force Falcons,93.9
Colorado State Rams,94.5
New Mexico Lobos,95.8
Boise State Broncos,97.1
Fresno State Bulldogs,99.8
Nevada Wolf Pack,100.2
San Jose State Spartans,103.9
Utah State Aggies,105.4

"""
mw_team_def_df = pd.read_csv(StringIO(mw_team_def_ratings))

mw_team_name_map = {
    # Mountain West Conference Teams in requested order
    'UNLV': 'UNLV Rebels',
    'San Diego St.': 'San Diego State Aztecs',
    'Wyoming': 'Wyoming Cowgirls',
    'Air Force': 'Air Force Falcons',
    'Colorado St.': 'Colorado State Rams',
    'New Mexico': 'New Mexico Lobos',
    'Boise St.': 'Boise State Broncos',
    'Fresno St.': 'Fresno State Bulldogs',
    'Nevada': 'Nevada Wolf Pack',
    'San Jose St.': 'San Jose State Spartans',
    'Utah St.': 'Utah State Aggies'
}
#------------------------------------
create a team name mapping using the following two lists : 

["Saint Mary's Gaels" 'Loyola Marymount Lions' 'San Francisco Dons'
 'Portland Pilots' 'Pacific Tigers' 'Santa Clara Broncos'
 'Pepperdine Waves' 'Gonzaga Bulldogs' 'San Diego Toreros']

['LMU (CA)' 'Pepperdine' 'Portland' 'San Diego' 'Pacific' 'Gonzaga'
 'San Francisco' 'Santa Clara' 'Oregon St.' 'Washington St.'
 "Saint Mary's (CA)"]

wcc_team_def_ratings = """
Team,TeamDefRtg
Saint Mary's Gaels, 94.9
Loyola Marymount Lions, 99.5
Portland Pilots, 90.5 
Pacific Tigers, 92.7
Santa Clara Broncos, 99.3
Pepperdine Waves, 98.9
Gonzaga Bulldogs, 95.9
San Diego Toreros, 99.8
San Francisco Dons, 92.6
Oregon State Beavers, 93.7 
Washington State Cougars, 95.3
"""



Lastly consider the python file to run the defensive analysis and an xgb model


# Load the data
df = pd.read_csv('.csv')  # Assuming you've saved the data

# Check basic info
print(f"Dataset shape: {df.shape}")
print("\nData types and missing values:")
print(df.info())


# Check missing values
print("\nMissing values per column:")
print(df.isnull().sum())

# Target Variable is Defensive Rating / Individual Difference 



# Imputation strategy:
# 1. For categorical: mode imputation
# 2. For numerical: median imputation (more robust to outliers)
# 3. For targets: These are what we'll predict, so we'll keep missing as is

from sklearn.impute import SimpleImputer

# Identify categorical columns
cat_cols = features.select_dtypes(include=['object']).columns
num_cols = features.select_dtypes(include=['number']).columns

# Impute categorical
cat_imputer = SimpleImputer(strategy='most_frequent')
features[cat_cols] = cat_imputer.fit_transform(features[cat_cols])

# Impute numerical
num_imputer = SimpleImputer(strategy='median')
features[num_cols] = num_imputer.fit_transform(features[num_cols])

# Verify no missing values remain in features
print("\nMissing values after imputation:")
print(features.isnull().sum().sum())  # Should be 0



# Potential feature engineering steps:
# 1. Convert height from inches to numerical (assuming '75' means 75 inches)
df['HEIGHT'] = df['HEIGHT'].astype(int)

# 2. Encode categorical variables
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
conference_ordering {'AAC':1,'WCC':2,'A10':3','MVC':4,'MWC':5,'IVY':6}

# For other categoricals: one-hot encoding
cat_cols_to_encode = ['Conference','Pos']
df_encoded = pd.get_dummies(df, columns=cat_cols_to_encode, drop_first=True)
df_encoded.head(3)

)



model = XGBRegressor(objective='reg:squarederror', n_estimators=200)

# Separate features and target
X = df_encoded.drop(columns=['Defensive Rating',  'Team'])  # Features
y = df_encoded['Defensive Rating']  # Target variable

# Split data (using only rows with defensive rating values)
known_data = df_encoded[df_encoded['Defensive Rating'].notna()]
X_train, X_test, y_train, y_test = train_test_split(
    known_data.drop(columns=['Defensive Rating', , 'Team','Player']),
    known_data['Defensive Rating'],
    test_size=0.2,
    random_state=42
)





#print(X_train.head())



# Define numeric vs categorical columns
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object', 'category']).columns


# 3. Initialize and train model (simpler version)
model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=200,
    random_state=42
)
xgb


# Fit the model
model.fit(X_train, y_train)

# Evaluate

y_pred = model.predict(X_test)
print(f"R2 Score: {r2_score(y_test, y_pred):.2f}")
print(f"RMSE: {root_mean_squared_error(y_test, y_pred):.2f}")



# Feature Importance
importances = model.feature_importances_
feature_importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': importances
}).sort_values('Importance', ascending=False)

print("\nTop 10 Important Features:")
print(feature_importance.head(10))
